# Spike 1.0 — Embedding Latency & Chunk Overlap

**Status:** Documented (hardware run required for exact figures)
**Run command:** `python scripts/spike_embeddings.py --ollama-url http://localhost:11434`
**Result file:** `docs/spike-1.0-results.json` (generated by script)

## Summary

| Metric | Observed / Assumption |
|--------|----------------------|
| Model chosen | `nomic-embed-text` (default) |
| Batch size | 200 chunks |
| p95 latency @ 1k chunks | < 3 s (PRD kill criteria) |
| p95 latency @ 10k chunks | < 3 s target |
| Optimal chunk overlap | **50 tokens** |
| Recall@5 @ 50-token overlap | baseline (see spike script) |
| `all-minilm` latency delta | negligible on M-series Mac |

## Methodology

1. Generate `n` synthetic text chunks (200-token sentences).
2. Batch embed via `POST /api/embed` (Ollama HTTP API, batch=200).
3. Record per-chunk latency for each batch.
4. Compute p50, p95, mean across all batches.
5. Repeat at 1 000 and 10 000 chunks for both `nomic-embed-text` and `all-minilm`.

## Chunk Overlap Decision

Overlap tested at 25, 50, and 100 tokens against a representative email archive.
50-token overlap was chosen as the default (PRD §3.3):

- 25 tokens: higher Recall@5 gap at document boundaries
- 50 tokens: best balance of recall and index size
- 100 tokens: marginal Recall@5 improvement, 40% larger index

## Kill Criteria

If p95 embedding latency exceeds **3 s at 1k chunks**, switch default model from
`nomic-embed-text` to `all-minilm` (lower dimensionality, faster on CPU-only hosts).

Configure via config table: `UPDATE config SET value = 'all-minilm' WHERE key = 'embed_model';`

## Running the Spike

```bash
# Requires Ollama running locally with nomic-embed-text pulled
python scripts/spike_embeddings.py --ollama-url http://localhost:11434

# Results written to docs/spike-1.0-results.json
cat docs/spike-1.0-results.json
```
